{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek71ZTQkLj__",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 align=center>Text Preprocssing</h1>\n",
    "\n",
    "There are different preprocessing steps depending on:\n",
    "  - **Language**\n",
    "      - English Normalization: \n",
    "                                lowercasing + stemming\n",
    "      - Arabich Normalization:\n",
    "                                 أآإؤ -> ا + remove diacritics + remove elongation + stemming\n",
    "  - **Problem Itself**\n",
    "      - Semantic Classification \n",
    "                                ✅ remove stopwords\n",
    "      - Translation:\n",
    "                                ❌ remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvI_RgoRQrMw"
   },
   "source": [
    "## **Some Preprocessing Steps**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 2140,
     "status": "ok",
     "timestamp": 1679689653290,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "hoiX0A_fTe_i"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVnoAfM9Q33v"
   },
   "source": [
    "### **English**\n",
    "#### text lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1679689189526,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "7nrM3O7UOm7C",
    "outputId": "e914df6a-9020-4501-e25f-18c024dc26e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Hello, Ahmed\n",
      "After : hello, ahmed\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, Ahmed'\n",
    "preprocessed_text = text.lower()\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGAQpra3Rym2"
   },
   "source": [
    "#### removing newlines and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1679689467550,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "nW6Mbdw7RqGH",
    "outputId": "c53c0fb0-6ce5-456d-e66c-288c3fa7cbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Hello, Ahmed\n",
      "this is Wssam\n",
      "\tI need your help.\n",
      "After : Hello, Ahmedthis is Wssam\tI need your help.\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, Ahmed\\nthis is Wssam\\n\\tI need your help.'\n",
    "preprocessed_text = text.replace('\\n', '')\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1679689421277,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "C1PTV06dSO1q",
    "outputId": "53cdc372-c3d3-4ee9-e683-5bd11adf092c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Hello, Ahmed\n",
      " this is Wssam\n",
      "\tI need your help.\n",
      "After : Hello, Ahmed\n",
      " this is Wssam\n",
      "I need your help.\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, Ahmed\\n this is Wssam\\n\\tI need your help.'\n",
    "preprocessed_text = text.replace('\\t', '')\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1679689440320,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "gZ0IKUA0So6l",
    "outputId": "90d4f5d9-fc5d-4b39-a84c-e2219006026e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Hello, Ahmed\n",
      " this is Wssam\n",
      "\tI need your help.\n",
      "After : Hello, Ahmed this is WssamI need your help.\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello, Ahmed\\n this is Wssam\\n\\tI need your help.'\n",
    "preprocessed_text = text.replace('\\n', '').replace('\\t', '')\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqT8Rej4S46z"
   },
   "source": [
    "#### removing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679690048052,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "zGbmlWnpVBtX"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1679689677626,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "OiZUTBTMStfF",
    "outputId": "8cd578a0-2d81-4081-e39e-2ef7fd001943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I recommend using regex101 website. visit it through: https://regex101.com/\n",
      "After : I recommend using regex101 website. visit it through: \n"
     ]
    }
   ],
   "source": [
    "text = 'I recommend using regex101 website. visit it through: https://regex101.com/'\n",
    "preprocessed_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BAfHaXGUw3f"
   },
   "source": [
    "#### removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1679690354495,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "01-uTcC6V1aZ"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679690359447,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "l_5VOxumTnf3",
    "outputId": "dd0467de-7478-41c6-88e2-f5db6deefe04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I'm happy, so I'll sleep early.\n",
      "After : Im happy so Ill sleep early\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm happy, so I'll sleep early.\"\n",
    "preprocessed_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkcGrpCYVo-l"
   },
   "source": [
    "#### removing contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "CmaxNKTmWHp1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: contractions in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1679690344244,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "e30lk0JNWJxF"
   },
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1679690344244,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "MFOypakTVnAD",
    "outputId": "577c05fa-c2fc-4e0c-9992-be6c91a7e2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I'm happy, so I'll sleep early.\n",
      "After : I am happy, so I will sleep early.\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm happy, so I'll sleep early.\"\n",
    "preprocessed_text = contractions.fix(text)\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtwhvCiEWQgV"
   },
   "source": [
    "> **The sequence of steps is VEEEEERY Important**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679690758803,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "Kvfg_S1WXucS"
   },
   "outputs": [],
   "source": [
    "text = \"I'm happy, so I'll sleep early.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679690759204,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "xRauV_liWYhB",
    "outputId": "d2128ca1-269c-43c3-8566-077c13a1e4fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I'm happy, so I'll sleep early.\n",
      "After : I am happy so I will sleep early\n"
     ]
    }
   ],
   "source": [
    "# remove contractions then remove punctuations\n",
    "preprocessed_text = contractions.fix(text)\n",
    "preprocessed_text = preprocessed_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679690759630,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "rxyJWse_Xtdi",
    "outputId": "9fe2a27c-88a9-48b0-e991-9a19b2039d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I'm happy, so I'll sleep early.\n",
      "After : I Am happy so Ill sleep early\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations then remove contractions\n",
    "preprocessed_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "preprocessed_text = contractions.fix(preprocessed_text)\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMjuvZu6XRDj"
   },
   "source": [
    "### **Arabic**\n",
    "#### remove diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1679690841376,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "luJqd3B-YCDM"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679690842887,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "FRvxUrIiX_x0"
   },
   "outputs": [],
   "source": [
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                            ـ    | #Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE) # re.VERBOSE --> add comments in string without compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679690853087,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "-htJuAVwW3Bx",
    "outputId": "be04b4c9-9de3-4f2f-e647-ab5bd75a840a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: الْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ\n",
      "After : الحمد لله رب العالمين\n"
     ]
    }
   ],
   "source": [
    "text = 'الْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ'\n",
    "preprocessed_text = re.sub(arabic_diacritics, '', text)\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DirLaz6-YQql"
   },
   "source": [
    "#### characters normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "M9KJ-DL5Y1jx"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1679691146908,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "4OlICvpqYGaZ",
    "outputId": "2d3cb081-bedf-43d4-d157-1585c1047f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: أنا الذي نظر الأعمى إلى أدبي\n",
      "After : انا الذي نظر الاعمي الي ادبي\n"
     ]
    }
   ],
   "source": [
    "text = 'أنا الذي نظر الأعمى إلى أدبي'\n",
    "\n",
    "preprocessed_text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "preprocessed_text = re.sub(\"ى\", \"ي\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"ؤ\", \"ء\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"ئ\", \"ء\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"ة\", \"ه\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"گ\", \"ك\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"ڤ\", \"ف\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"چ\", \"ج\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"ژ\", \"ز\", preprocessed_text)\n",
    "preprocessed_text = re.sub(\"پ\", \"ب\", preprocessed_text)\n",
    "\n",
    "print(f'Before: {text}')\n",
    "print(f'After : {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1F2lOCrZVBs"
   },
   "source": [
    "### **More Steps:** Google it\n",
    "  - remove stopwords ((( Watch out **the steps** )))\n",
    "  - remove usernames & tags\n",
    "  - remove emojis\n",
    "  - remove numbers\n",
    "  - remove text elongation \n",
    "  - remove extra whitespaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1679692116875,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "yPz8dg36aJYp",
    "outputId": "47e4dc78-cd6b-4ea8-ea45-7263c4ec6c1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1679692127084,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "KvDuhSRVZOF5",
    "outputId": "bdbcea63-6916-4539-8d82-bc88b84e400b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1679692141108,
     "user": {
      "displayName": "Wssam Hassan",
      "userId": "16924058918477820692"
     },
     "user_tz": -120
    },
    "id": "aJK-ERj6dAAB",
    "outputId": "dbf1794c-d17a-439f-f5e2-afd8be55aece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NLmKbsKb-Bi"
   },
   "source": [
    "[English Stopwords](https://gist.github.com/sebleier/554280) <br>\n",
    "[Arabic Stopwords](https://github.com/mohataher/arabic-stop-words/blob/master/list.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>All Text Preprocssing Steps</h1>\n",
    "\n",
    "# Dataset labeled datasset collected from twitter\n",
    "\n",
    "- Objective classify tweets containing hate speech from other tweets.\n",
    "    - 0 -> no hate speech\n",
    "    - 1 -> contains hate speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLTK data (only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # or nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Remove mentions, hashtags, URLs, and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub(r'@\\w+', '', text)                   # remove mentions (@user)\n",
    "    text = re.sub(r'#\\w+', '', text)                   # remove hashtags (#word)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)         # remove links\n",
    "    text = re.sub(r'\\d+', '', text)                    # remove numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F\\u0600-\\u06FF]', '', text)  # remove emojis & special chars\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Remove emojis and non-ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    #return [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if t not in stop_words and len(t) > 1:\n",
    "            filtered_tokens.append(t)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(tokens):\n",
    "    #return [stemmer.stem(t) for t in tokens]\n",
    "    stemmed_tokens = []\n",
    "    for t in tokens:\n",
    "        stemmed_word = stemmer.stem(t)\n",
    "        stemmed_tokens.append(stemmed_word)\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "    #return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    lemmatized_tokens = []\n",
    "    for t in tokens:\n",
    "        lemma = lemmatizer.lemmatize(t)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Preprocessing Function (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = remove_noise(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_stemming(tokens)\n",
    "    tokens = apply_lemmatization(tokens)\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      31962 non-null  int64 \n",
      " 1   label   31962 non-null  int64 \n",
      " 2   tweet   31962 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv') # create a DataFrame \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.ð...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       31962\n",
       "label    31962\n",
       "tweet    31962\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NaNs\n",
    "df.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2432"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicates\n",
    "df['tweet'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.ð...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop Duplicate\n",
    "df.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17488    i may call her my wife for 3 years allready!  ...\n",
      "18250      gave my dad his #fathersdaygift early #lovei...\n",
      "17626     factsguide: the magic realism by rob gonsalve...\n",
      "9950     wow    just passed 70,000 views #grateful   #t...\n",
      "11919                   i have a bone to pick with y'all  \n",
      "26444    r.i.p literally, you will be missed and the ma...\n",
      "23823    omg finally after 2 months of waiting im watch...\n",
      "26915    scream 2Ã04 promo âhappy bÃ­hday to meâ (...\n",
      "27764     creating something that brings happiness to o...\n",
      "11851    rip anton yelchin :( such a great young actor....\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# how samples of data texts to find out required preprocessing steps\n",
    "#df.head(10)\n",
    "print(df['tweet'].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Apply preprocessing on 'tweet' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned dataset saved as 'cleaned_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\" Cleaned dataset saved as 'cleaned_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Show sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample before and after preprocessing:\n",
      "\n",
      "Original:  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Cleaned : father dysfunct selfish drag kid dysfunct\n",
      "--------------------------------------------------------------------------------\n",
      "Original: @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "Cleaned : thank credit cant use caus dont offer wheelchair van pdx\n",
      "--------------------------------------------------------------------------------\n",
      "Original:   bihday your majesty\n",
      "Cleaned : bihday majesti\n",
      "--------------------------------------------------------------------------------\n",
      "Original: #model   i love u take with u all the time in urð±!!! ðððð",
      "ð¦ð¦ð¦  \n",
      "Cleaned : love take time ur\n",
      "--------------------------------------------------------------------------------\n",
      "Original:  factsguide: society now    #motivation\n",
      "Cleaned : factsguid societi\n",
      "--------------------------------------------------------------------------------\n",
      "Original: [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  \n",
      "Cleaned : huge fan fare big talk leav chao pay disput get\n",
      "--------------------------------------------------------------------------------\n",
      "Original:  @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦\n",
      "Cleaned : camp tomorrow danni\n",
      "--------------------------------------------------------------------------------\n",
      "Original: the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl\n",
      "Cleaned : next school year year exam cant think\n",
      "--------------------------------------------------------------------------------\n",
      "Original: we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦ \n",
      "Cleaned : love land\n",
      "--------------------------------------------------------------------------------\n",
      "Original:  @user @user welcome here !  i'm   it's so #gr8 ! \n",
      "Cleaned : welcom im\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Sample before and after preprocessing:\\n\")\n",
    "for i in range(10):\n",
    "    print(\"Original:\", df['tweet'][i])\n",
    "    print(\"Cleaned :\", df['cleaned_tweet'][i])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Exercise: Text Preprocessing for Product Reviews Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "- The goal of this task is to build a complete text preprocessing pipeline for cleaning and preparing customer product reviews before using them in sentiment analysis or other NLP models.\n",
    "\n",
    "- You will apply a series of text-cleaning and normalization techniques to ensure that the raw text data becomes structured, uniform, and ready for further processing or model training.\n",
    "\n",
    "# Background:\n",
    "- In Natural Language Processing (NLP), raw text often contains noise, such as punctuation, emojis, URLs, numbers, or common stopwords that do not contribute to the semantic meaning.\n",
    "\n",
    "# Exercise Steps\n",
    "- In this project, you will build a complete code to clean product review data from a CSV file.\n",
    "- Search for product_reviews.csv OR CREATE it:\n",
    "    - The dataset contains customer reviews of various products, along with their corresponding ratings.\n",
    "        - **Column Name\tDescription**\n",
    "            - **review_id:** Unique identifier for each review\n",
    "            - **rating:** Numerical rating from 1 to 5\n",
    "            - **review_text:** The text of the customer review (raw data)\n",
    "        - Use pandas to read the dataset from product_reviews.csv.\n",
    "        - Verify that the column names are correct and that there are no missing values in review_text.\n",
    "- Apply all stages of text processing to it (cleaning, splitting, standardization, stopword removal, stemming, and lemmatization).\n",
    "- Then save the cleaned data in a new file for use in sentiment analysis or any other NLP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
