{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NLmKbsKb-Bi"
   },
   "source": [
    "[English Stopwords](https://gist.github.com/sebleier/554280) <br>\n",
    "[Arabic Stopwords](https://github.com/mohataher/arabic-stop-words/blob/master/list.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>All Text Preprocssing Steps</h1>\n",
    "\n",
    "# Dataset labeled datasset collected from twitter\n",
    "\n",
    "- Objective classify tweets containing hate speech from other tweets.\n",
    "    - 0 -> no hate speech\n",
    "    - 1 -> contains hate speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download NLTK data (only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\nlp_offline\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # or nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Remove mentions, hashtags, URLs, and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "    text = re.sub(r'@\\w+', '', text)                   # remove mentions (@user)\n",
    "    text = re.sub(r'#\\w+', '', text)                   # remove hashtags (#word)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)         # remove links\n",
    "    text = re.sub(r'\\d+', '', text)                    # remove numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F\\u0600-\\u06FF]', '', text)  # remove emojis & special chars\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Remove emojis and non-ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    #return [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if t not in stop_words and len(t) > 1:\n",
    "            filtered_tokens.append(t)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(tokens):\n",
    "    #return [stemmer.stem(t) for t in tokens]\n",
    "    stemmed_tokens = []\n",
    "    for t in tokens:\n",
    "        stemmed_word = stemmer.stem(t)\n",
    "        stemmed_tokens.append(stemmed_word)\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(tokens):\n",
    "    #return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    lemmatized_tokens = []\n",
    "    for t in tokens:\n",
    "        lemma = lemmatizer.lemmatize(t)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Preprocessing Function (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = remove_noise(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_stemming(tokens)\n",
    "    tokens = apply_lemmatization(tokens)\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      31962 non-null  int64 \n",
      " 1   label   31962 non-null  int64 \n",
      " 2   tweet   31962 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv') # create a DataFrame \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.ð...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       31962\n",
       "label    31962\n",
       "tweet    31962\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NaNs\n",
    "df.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2432"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicates\n",
    "df['tweet'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.ð...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop Duplicate\n",
    "df.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21605    gonna spend time with the kids an mom.w  well ...\n",
      "16637    #dailyaffirmation i do not #exist to #impress ...\n",
      "17389     @user @user i'm stepping down as a @user boar...\n",
      "22859    sorria ðâº  #boooomdia #quaafeira #atualiz...\n",
      "14726    siri reveals what apple will announce at wwdc,...\n",
      "25173    i would never in my life allow my future kids ...\n",
      "20002     @user leaving day at ih belfast!    #ihbelfas...\n",
      "27890             dad #fucks #small #teen #babysitter mp4 \n",
      "3976        finally cleared my storage!! #glad #finally   \n",
      "10614    angry owls #owls #aworks #abstracta #colorful ...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# how samples of data texts to find out required preprocessing steps\n",
    "#df.head(10)\n",
    "print(df['tweet'].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Apply preprocessing on 'tweet' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned dataset saved as 'cleaned_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\" Cleaned dataset saved as 'cleaned_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Show sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample before and after preprocessing:\n",
      "\n",
      "Original:  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Cleaned : father dysfunct selfish drag kid dysfunct\n",
      "--------------------------------------------------------------------------------\n",
      "Original: @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\n",
      "Cleaned : thank credit cant use caus dont offer wheelchair van pdx\n",
      "--------------------------------------------------------------------------------\n",
      "Original:   bihday your majesty\n",
      "Cleaned : bihday majesti\n",
      "--------------------------------------------------------------------------------\n",
      "Original: #model   i love u take with u all the time in urð±!!! ðððð",
      "ð¦ð¦ð¦  \n",
      "Cleaned : love take time ur\n",
      "--------------------------------------------------------------------------------\n",
      "Original:  factsguide: society now    #motivation\n",
      "Cleaned : factsguid societi\n",
      "--------------------------------------------------------------------------------\n",
      "Original: [2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  \n",
      "Cleaned : huge fan fare big talk leav chao pay disput get\n",
      "--------------------------------------------------------------------------------\n",
      "Original:  @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦\n",
      "Cleaned : camp tomorrow danni\n",
      "--------------------------------------------------------------------------------\n",
      "Original: the next school year is the year for exams.ð¯ can't think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl\n",
      "Cleaned : next school year year exam cant think\n",
      "--------------------------------------------------------------------------------\n",
      "Original: we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦ \n",
      "Cleaned : love land\n",
      "--------------------------------------------------------------------------------\n",
      "Original:  @user @user welcome here !  i'm   it's so #gr8 ! \n",
      "Cleaned : welcom im\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Sample before and after preprocessing:\\n\")\n",
    "for i in range(10):\n",
    "    print(\"Original:\", df['tweet'][i])\n",
    "    print(\"Cleaned :\", df['cleaned_tweet'][i])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Exercise: Text Preprocessing for Product Reviews Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "- The goal of this task is to build a complete text preprocessing pipeline for cleaning and preparing customer product reviews before using them in sentiment analysis or other NLP models.\n",
    "\n",
    "- You will apply a series of text-cleaning and normalization techniques to ensure that the raw text data becomes structured, uniform, and ready for further processing or model training.\n",
    "\n",
    "# Background:\n",
    "- In Natural Language Processing (NLP), raw text often contains noise, such as punctuation, emojis, URLs, numbers, or common stopwords that do not contribute to the semantic meaning.\n",
    "\n",
    "# Exercise Steps\n",
    "- In this project, you will build a complete code to clean product review data from a CSV file.\n",
    "- Search for product_reviews.csv OR CREATE it:\n",
    "    - The dataset contains customer reviews of various products, along with their corresponding ratings.\n",
    "        - **Column Name\tDescription**\n",
    "            - **review_id:** Unique identifier for each review\n",
    "            - **rating:** Numerical rating from 1 to 5\n",
    "            - **review_text:** The text of the customer review (raw data)\n",
    "        - Use pandas to read the dataset from product_reviews.csv.\n",
    "        - Verify that the column names are correct and that there are no missing values in review_text.\n",
    "- Apply all stages of text processing to it (cleaning, splitting, standardization, stopword removal, stemming, and lemmatization).\n",
    "- Then save the cleaned data in a new file for use in sentiment analysis or any other NLP task."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
